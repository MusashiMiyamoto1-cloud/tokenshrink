{"chunks": [{"text": "# TokenShrink **Cut your AI costs 50-80%.** FAISS semantic retrieval + LLMLingua compression. Stop loading entire files into your prompts. Load only what's relevant, compressed. ## Quick Start ```bash pip install tokenshrink # Index your docs tokenshrink index ./docs # Get compressed context tokenshrink query \"What are the API limits?\" --compress ``` ## Why TokenShrink? | Without | With TokenShrink | |---------|------------------| | Load entire file (5000 tokens) | Load relevant chunks (200 tokens) | | $0.15 per query | $0.03 per query | | Slow responses | Fast responses | | Hit context limits | Stay under limits | **Real numbers:** 50-80% token reduction on typical RAG workloads. ## Installation ```bash # Basic (retrieval only) pip install tokenshrink # With compression (recommended) pip install tokenshrink[compression] ``` ## Usage ### CLI ```bash # Index files tokenshrink index ./docs tokenshrink index ./src --extensions .py,.md # Query (retrieval only) tokenshrink query \"How do I authenticate?\" # Query with compression tokenshrink query \"How do I authenticate?\" --compress # View stats tokenshrink stats # JSON output (for scripts) tokenshrink query \"question\" --json ``` ### Python API ```python from tokenshrink import TokenShrink # Initialize ts = TokenShrink() # Index your files ts.index(\"./docs\") # Get compressed context result = ts.query(\"What are the rate limits?\") print(result.context) # Ready for your LLM print(result.savings) # \"Saved 65% (1200 \u2192 420 tokens)\" print(result.sources) # [\"api.md\", \"limits.md\"] ``` ### Integration Examples **With OpenAI:** ```python from tokenshrink import TokenShrink from openai import OpenAI ts = TokenShrink() ts.index(\"./knowledge\") client = OpenAI() def ask(question: str) -> str: # Get relevant, compressed context ctx = ts.query(question) response = client.chat.completions.create( model=\"gpt-4\", messages=[ {\"role\": \"system\", \"content\": f\"Context:\\n{ctx.context}\"}, {\"role\": \"user\", \"content\": question} ] ) print(f\"Token savings: {ctx.savings}\") return response.choices[0].message.content answer = ask(\"What's the refund policy?\") ``` **With LangChain:** ```python from tokenshrink import TokenShrink from langchain.llms import OpenAI from langchain.prompts import PromptTemplate ts = TokenShrink() ts.index(\"./docs\") def get_context(query: str) -> str: result = ts.query(query) return result.context # Use in your chain template = PromptTemplate( input_variables=[\"context\", \"question\"], template=\"Context:\\n{context}\\n\\nQuestion: {question}\" ) ``` ## How It Works ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Files \u2502 \u2500\u2500\u25ba \u2502 Indexer \u2502 \u2500\u2500\u25ba \u2502 FAISS Index\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (MiniLM) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Question \u2502 \u2500\u2500\u25ba \u2502 Search \u2502 \u2500\u2500\u25ba \u2502 Relevant \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 Chunks \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Compressor \u2502 \u2502 (LLMLingua-2) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Optimized \u2502 \u2502 Context \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` 1. **Index**: Chunks your files, creates embeddings with MiniLM 2. **Search**: Finds relevant chunks via semantic similarity 3. **Compress**: Removes redundancy while preserving meaning ## Configuration ```python ts = TokenShrink( index_dir=\".tokenshrink\", # Where to store the index model=\"all-MiniLM-L6-v2\", # Embedding model chunk_size=512, # Words per chunk chunk_overlap=50, # Overlap between chunks device=\"auto\", # auto, mps, cuda, cpu compression=True, # Enable LLMLingua ) ``` ## Supported File Types Default: `.md`, `.txt`, `.py`, `.json`, `.yaml`, `.yml` Custom: ```bash tokenshrink index ./src --extensions .py,.ts,.js,.md ``` ## Performance | Metric | Value | |--------|-------| | Index 1000 files | ~30 seconds | | Search latency | <50ms | | Compression | ~200ms | | Token", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/README.md", "offset": 0}, {"text": ") ``` ## Supported File Types Default: `.md`, `.txt`, `.py`, `.json`, `.yaml`, `.yml` Custom: ```bash tokenshrink index ./src --extensions .py,.ts,.js,.md ``` ## Performance | Metric | Value | |--------|-------| | Index 1000 files | ~30 seconds | | Search latency | <50ms | | Compression | ~200ms | | Token reduction | 50-80% | ## Requirements - Python 3.10+ - 4GB RAM (8GB for compression) - Apple Silicon: MPS acceleration - NVIDIA: CUDA acceleration ## FAQ **Q: Do I need LLMLingua?** A: No. Retrieval works without it (still saves 60-70% by loading only relevant chunks). Add compression for extra 20-30% savings. **Q: Does it work with non-English?** A: Retrieval works well with multilingual content. Compression is English-optimized. **Q: How do I update the index?** A: Just run `tokenshrink index` again. It detects changed files automatically. ## Uninstall ```bash pip uninstall tokenshrink rm -rf .tokenshrink # Remove local index ``` --- Built by [Musashi](https://github.com/MusashiMiyamoto1-cloud) \u00b7 Part of [Agent Guard](https://agentguard.co)", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/README.md", "offset": 462}, {"text": "\"\"\" TokenShrink: Cut your AI costs 50-80%. FAISS semantic retrieval + LLMLingua compression for token-efficient context loading. Usage: from tokenshrink import TokenShrink ts = TokenShrink() ts.index(\"./docs\") result = ts.query(\"What are the API limits?\") print(result.context) # Compressed, relevant context print(result.savings) # \"Saved 65% (1200 \u2192 420 tokens)\" CLI: tokenshrink index ./docs tokenshrink query \"your question\" tokenshrink stats \"\"\" from tokenshrink.pipeline import TokenShrink, ShrinkResult __version__ = \"0.1.0\" __all__ = [\"TokenShrink\", \"ShrinkResult\"]", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/__init__.py", "offset": 0}, {"text": "\"\"\" TokenShrink CLI. Usage: tokenshrink index ./docs tokenshrink query \"your question\" tokenshrink stats tokenshrink clear \"\"\" import argparse import sys import json from pathlib import Path from tokenshrink import TokenShrink, __version__ def main(): parser = argparse.ArgumentParser( prog=\"tokenshrink\", description=\"Cut your AI costs 50-80%. FAISS retrieval + LLMLingua compression.\", ) parser.add_argument(\"--version\", action=\"version\", version=f\"tokenshrink {__version__}\") parser.add_argument( \"--index-dir\", default=\".tokenshrink\", help=\"Directory to store the index (default: .tokenshrink)\", ) parser.add_argument( \"--json\", action=\"store_true\", help=\"Output as JSON\", ) subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\") # index index_parser = subparsers.add_parser(\"index\", help=\"Index files for retrieval\") index_parser.add_argument(\"path\", help=\"File or directory to index\") index_parser.add_argument( \"-e\", \"--extensions\", default=\".md,.txt,.py,.json,.yaml,.yml\", help=\"File extensions to include (comma-separated)\", ) index_parser.add_argument( \"-f\", \"--force\", action=\"store_true\", help=\"Re-index even if files unchanged\", ) # query query_parser = subparsers.add_parser(\"query\", help=\"Get relevant context for a question\") query_parser.add_argument(\"question\", help=\"Your question\") query_parser.add_argument( \"-k\", type=int, default=5, help=\"Number of chunks to retrieve (default: 5)\", ) query_parser.add_argument( \"-c\", \"--compress\", action=\"store_true\", help=\"Enable compression (requires llmlingua)\", ) query_parser.add_argument( \"--no-compress\", action=\"store_true\", help=\"Disable compression\", ) query_parser.add_argument( \"--max-tokens\", type=int, default=2000, help=\"Target token limit (default: 2000)\", ) # search (alias for query without compression) search_parser = subparsers.add_parser(\"search\", help=\"Search without compression\") search_parser.add_argument(\"question\", help=\"Your question\") search_parser.add_argument( \"-k\", type=int, default=5, help=\"Number of chunks to retrieve (default: 5)\", ) # stats subparsers.add_parser(\"stats\", help=\"Show index statistics\") # clear subparsers.add_parser(\"clear\", help=\"Clear the index\") args = parser.parse_args() if not args.command: parser.print_help() sys.exit(0) # Determine compression setting compression = True if hasattr(args, 'no_compress') and args.no_compress: compression = False if hasattr(args, 'compress') and args.compress: compression = True ts = TokenShrink( index_dir=args.index_dir, compression=compression, ) if args.command == \"index\": extensions = tuple(e.strip() if e.startswith(\".\") else f\".{e.strip()}\" for e in args.extensions.split(\",\")) result = ts.index(args.path, extensions=extensions, force=args.force) if args.json: print(json.dumps(result, indent=2)) else: print(f\"\u2713 Indexed {result['files_indexed']} files\") print(f\" Chunks: {result['chunks_added']} added, {result['total_chunks']} total\") print(f\" Files: {result['total_files']} tracked\") elif args.command == \"query\": compress = None if args.compress: compress = True elif args.no_compress: compress = False result = ts.query( args.question, k=args.k, max_tokens=args.max_tokens, compress=compress, ) if args.json: print(json.dumps({ \"context\": result.context, \"sources\": result.sources, \"original_tokens\": result.original_tokens, \"compressed_tokens\": result.compressed_tokens, \"savings_pct\": result.savings_pct, }, indent=2)) else: if result.sources: print(f\"Sources: {', '.join(Path(s).name for s in result.sources)}\") print(f\"Stats: {result.savings}\") print() print(result.context) else: print(\"No relevant content found.\") elif args.command == \"search\": results = ts.search(args.question, k=args.k) if args.json: print(json.dumps(results, indent=2)) else: if not results: print(\"No results found.\") else: for i, r in enumerate(results, 1): print(f\"\\n[{i}] {Path(r['source']).name} (score: {r['score']:.3f})\") print(\"-\" * 40) print(r[\"text\"][:500] + (\"...\" if len(r[\"text\"]) > 500 else \"\")) elif args.command == \"stats\": result = ts.stats() if args.json: print(json.dumps(result, indent=2)) else: print(f\"Index: {result['index_dir']}\") print(f\"Chunks: {result['total_chunks']}\") print(f\"Files: {result['total_files']}\") print(f\"Compression: {'available' if result['compression_available'] else 'not installed'}\") print(f\"Device: {result['device']}\") elif args.command == \"clear\": ts.clear() if args.json: print(json.dumps({\"status\": \"cleared\"})) else: print(\"\u2713 Index cleared\") if __name__ == \"__main__\": main()", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/cli.py", "offset": 0}, {"text": "\"\"\" TokenShrink core: FAISS retrieval + LLMLingua compression. \"\"\" import os import json import hashlib from pathlib import Path from dataclasses import dataclass from typing import Optional import faiss import numpy as np from sentence_transformers import SentenceTransformer # Optional compression try: from llmlingua import PromptCompressor HAS_COMPRESSION = True except ImportError: HAS_COMPRESSION = False @dataclass class ShrinkResult: \"\"\"Result from a query.\"\"\" context: str sources: list[str] original_tokens: int compressed_tokens: int ratio: float @property def savings(self) -> str: pct = (1 - self.ratio) * 100 return f\"Saved {pct:.0f}% ({self.original_tokens} \u2192 {self.compressed_tokens} tokens)\" @property def savings_pct(self) -> float: return (1 - self.ratio) * 100 class TokenShrink: \"\"\" Token-efficient context loading. Usage: ts = TokenShrink() ts.index(\"./docs\") result = ts.query(\"What are the constraints?\") print(result.context) \"\"\" def __init__( self, index_dir: Optional[str] = None, model: str = \"all-MiniLM-L6-v2\", chunk_size: int = 512, chunk_overlap: int = 50, device: str = \"auto\", compression: bool = True, ): \"\"\" Initialize TokenShrink. Args: index_dir: Where to store the FAISS index. Default: ./.tokenshrink model: Sentence transformer model for embeddings. chunk_size: Words per chunk. chunk_overlap: Overlap between chunks. device: Device for compression (auto, mps, cuda, cpu). compression: Enable LLMLingua compression. \"\"\" self.index_dir = Path(index_dir or \".tokenshrink\") self.chunk_size = chunk_size self.chunk_overlap = chunk_overlap self._compression_enabled = compression and HAS_COMPRESSION # Auto-detect device if device == \"auto\": import torch if torch.backends.mps.is_available(): device = \"mps\" elif torch.cuda.is_available(): device = \"cuda\" else: device = \"cpu\" self._device = device # Load embedding model self._model = SentenceTransformer(model) self._dim = self._model.get_sentence_embedding_dimension() # FAISS index self._index = faiss.IndexFlatIP(self._dim) self._chunks: list[dict] = [] self._file_hashes: dict[str, str] = {} # Load existing index if self.index_dir.exists(): self._load() # Lazy-load compressor self._compressor: Optional[PromptCompressor] = None def _get_compressor(self) -> PromptCompressor: \"\"\"Lazy-load the compressor.\"\"\" if self._compressor is None: if not HAS_COMPRESSION: raise ImportError( \"Compression requires llmlingua. \" \"Install with: pip install tokenshrink[compression]\" ) self._compressor = PromptCompressor( model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\", use_llmlingua2=True, device_map=self._device, ) return self._compressor def _chunk_text(self, text: str, source: str) -> list[dict]: \"\"\"Split text into overlapping chunks.\"\"\" words = text.split() chunks = [] for i in range(0, len(words), self.chunk_size - self.chunk_overlap): chunk_words = words[i:i + self.chunk_size] if len(chunk_words) < 20: continue chunks.append({ \"text\": \" \".join(chunk_words), \"source\": source, \"offset\": i, }) return chunks def _hash_file(self, path: Path) -> str: \"\"\"Get file content hash.\"\"\" with open(path, \"rb\") as f: return hashlib.md5(f.read()).hexdigest() def index( self, path: str, extensions: tuple[str, ...] = (\".md\", \".txt\", \".py\", \".json\", \".yaml\", \".yml\"), force: bool = False, ) -> dict: \"\"\" Index files for retrieval. Args: path: File or directory to index. extensions: File extensions to include (for directories). force: Re-index even if unchanged. Returns: Stats dict with files_indexed, chunks_added, total_chunks. \"\"\" path = Path(path) skip_dirs = {\"node_modules\", \"__pycache__\", \".venv\", \"venv\", \".git\", \".tokenshrink\"} files_indexed = 0 chunks_added = 0 if path.is_file(): files = [path] else: files = [ f for f in path.rglob(\"*\") if f.is_file() and f.suffix.lower() in extensions and not f.name.startswith(\".\") and not any(d in f.parts for d in skip_dirs) ] for file_path in files: try: file_str = str(file_path.resolve()) current_hash = self._hash_file(file_path) if not force and self._file_hashes.get(file_str) == current_hash: continue with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: text = f.read() chunks = self._chunk_text(text, file_str) if not chunks: continue embeddings = self._model.encode(", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/pipeline.py", "offset": 0}, {"text": "and not any(d in f.parts for d in skip_dirs) ] for file_path in files: try: file_str = str(file_path.resolve()) current_hash = self._hash_file(file_path) if not force and self._file_hashes.get(file_str) == current_hash: continue with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: text = f.read() chunks = self._chunk_text(text, file_str) if not chunks: continue embeddings = self._model.encode( [c[\"text\"] for c in chunks], normalize_embeddings=True ) self._index.add(np.array(embeddings, dtype=np.float32)) self._chunks.extend(chunks) self._file_hashes[file_str] = current_hash files_indexed += 1 chunks_added += len(chunks) except Exception as e: print(f\"Warning: {file_path}: {e}\") self._save() return { \"files_indexed\": files_indexed, \"chunks_added\": chunks_added, \"total_chunks\": self._index.ntotal, \"total_files\": len(self._file_hashes), } def query( self, question: str, k: int = 5, min_score: float = 0.3, max_tokens: int = 2000, compress: Optional[bool] = None, ) -> ShrinkResult: \"\"\" Get relevant, compressed context for a question. Args: question: The query. k: Number of chunks to retrieve. min_score: Minimum similarity score (0-1). max_tokens: Target token limit for compression. compress: Override compression setting. Returns: ShrinkResult with context, sources, and token stats. \"\"\" if self._index.ntotal == 0: return ShrinkResult( context=\"\", sources=[], original_tokens=0, compressed_tokens=0, ratio=1.0, ) # Retrieve embedding = self._model.encode([question], normalize_embeddings=True) scores, indices = self._index.search( np.array(embedding, dtype=np.float32), min(k, self._index.ntotal) ) results = [] for score, idx in zip(scores[0], indices[0]): if idx >= 0 and score >= min_score: chunk = self._chunks[idx].copy() chunk[\"score\"] = float(score) results.append(chunk) if not results: return ShrinkResult( context=\"\", sources=[], original_tokens=0, compressed_tokens=0, ratio=1.0, ) # Combine chunks combined = \"\\n\\n---\\n\\n\".join( f\"[{Path(c['source']).name}]\\n{c['text']}\" for c in results ) sources = list(set(c[\"source\"] for c in results)) # Estimate tokens original_tokens = len(combined.split()) # Compress if enabled should_compress = compress if compress is not None else self._compression_enabled if should_compress and original_tokens > 100: compressed, stats = self._compress(combined, max_tokens) return ShrinkResult( context=compressed, sources=sources, original_tokens=stats[\"original\"], compressed_tokens=stats[\"compressed\"], ratio=stats[\"ratio\"], ) return ShrinkResult( context=combined, sources=sources, original_tokens=original_tokens, compressed_tokens=original_tokens, ratio=1.0, ) def _compress(self, text: str, max_tokens: int) -> tuple[str, dict]: \"\"\"Compress text using LLMLingua-2.\"\"\" compressor = self._get_compressor() # LLMLingua-2 works best with smaller chunks max_chars = 1500 est_tokens = len(text.split()) target_ratio = min(0.9, max_tokens / est_tokens) if est_tokens else 0.5 if len(text) <= max_chars: result = compressor.compress_prompt( text, rate=target_ratio, force_tokens=[\"\\n\", \".\", \"!\", \"?\"], ) return result[\"compressed_prompt\"], { \"original\": result[\"origin_tokens\"], \"compressed\": result[\"compressed_tokens\"], \"ratio\": result[\"compressed_tokens\"] / result[\"origin_tokens\"], } # Chunk large texts parts = [text[i:i+max_chars] for i in range(0, len(text), max_chars)] compressed_parts = [] total_original = 0 total_compressed = 0 for part in parts: if not part.strip(): continue r = compressor.compress_prompt(part, rate=target_ratio) compressed_parts.append(r[\"compressed_prompt\"]) total_original += r[\"origin_tokens\"] total_compressed += r[\"compressed_tokens\"] return \" \".join(compressed_parts), { \"original\": total_original, \"compressed\": total_compressed, \"ratio\": total_compressed / total_original if total_original else 1.0, } def search(self, question: str, k: int = 5, min_score: float = 0.3) -> list[dict]: \"\"\"Search without compression. Returns raw chunks with scores.\"\"\" if self._index.ntotal == 0: return [] embedding = self._model.encode([question], normalize_embeddings=True) scores, indices = self._index.search( np.array(embedding, dtype=np.float32), min(k, self._index.ntotal) ) results = [] for score, idx in zip(scores[0], indices[0]): if idx >= 0 and score >= min_score: chunk = self._chunks[idx].copy() chunk[\"score\"] = float(score) results.append(chunk) return results def stats(self) -> dict: \"\"\"Get index statistics.\"\"\" return { \"total_chunks\": self._index.ntotal, \"total_files\": len(self._file_hashes), \"index_dir\": str(self.index_dir), \"compression_available\": HAS_COMPRESSION, \"compression_enabled\": self._compression_enabled, \"device\": self._device, } def clear(self): \"\"\"Clear the index.\"\"\" self._index = faiss.IndexFlatIP(self._dim) self._chunks = [] self._file_hashes = {} if self.index_dir.exists(): import shutil", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/pipeline.py", "offset": 462}, {"text": "min_score: chunk = self._chunks[idx].copy() chunk[\"score\"] = float(score) results.append(chunk) return results def stats(self) -> dict: \"\"\"Get index statistics.\"\"\" return { \"total_chunks\": self._index.ntotal, \"total_files\": len(self._file_hashes), \"index_dir\": str(self.index_dir), \"compression_available\": HAS_COMPRESSION, \"compression_enabled\": self._compression_enabled, \"device\": self._device, } def clear(self): \"\"\"Clear the index.\"\"\" self._index = faiss.IndexFlatIP(self._dim) self._chunks = [] self._file_hashes = {} if self.index_dir.exists(): import shutil shutil.rmtree(self.index_dir) def _save(self): \"\"\"Save index to disk.\"\"\" self.index_dir.mkdir(parents=True, exist_ok=True) faiss.write_index(self._index, str(self.index_dir / \"index.faiss\")) with open(self.index_dir / \"meta.json\", \"w\") as f: json.dump({ \"chunks\": self._chunks, \"hashes\": self._file_hashes, }, f) def _load(self): \"\"\"Load index from disk.\"\"\" index_path = self.index_dir / \"index.faiss\" meta_path = self.index_dir / \"meta.json\" if index_path.exists(): self._index = faiss.read_index(str(index_path)) if meta_path.exists(): with open(meta_path) as f: data = json.load(f) self._chunks = data.get(\"chunks\", []) self._file_hashes = data.get(\"hashes\", {})", "source": "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/pipeline.py", "offset": 924}], "hashes": {"/Users/musashi/.openclaw/workspace/tokenshrink/README.md": "467ef856e4f8ddc7aa4d5f1828f51ff3", "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/__init__.py": "aee71e420d0901af03f8debc6f0f2841", "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/cli.py": "fef08a4a79a70bd470ae0ac1f3f508ad", "/Users/musashi/.openclaw/workspace/tokenshrink/src/tokenshrink/pipeline.py": "91f7e0b7fb8ed9ca24052b8105aab345"}}